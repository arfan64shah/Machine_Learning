{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d1038b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first read the data using pandas library\n",
    "import pandas as pd\n",
    "dataset = pd.read_csv('Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f9a62e03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>fractal_dimension_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "      <th>diagnosis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>0.1726</td>\n",
       "      <td>0.05623</td>\n",
       "      <td>...</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>0.1752</td>\n",
       "      <td>0.05533</td>\n",
       "      <td>...</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>0.05648</td>\n",
       "      <td>...</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>0.07016</td>\n",
       "      <td>...</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1587</td>\n",
       "      <td>0.05884</td>\n",
       "      <td>...</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     radius_mean  texture_mean  perimeter_mean  area_mean  smoothness_mean  \\\n",
       "0          17.99         10.38          122.80     1001.0          0.11840   \n",
       "1          20.57         17.77          132.90     1326.0          0.08474   \n",
       "2          19.69         21.25          130.00     1203.0          0.10960   \n",
       "3          11.42         20.38           77.58      386.1          0.14250   \n",
       "4          20.29         14.34          135.10     1297.0          0.10030   \n",
       "..           ...           ...             ...        ...              ...   \n",
       "564        21.56         22.39          142.00     1479.0          0.11100   \n",
       "565        20.13         28.25          131.20     1261.0          0.09780   \n",
       "566        16.60         28.08          108.30      858.1          0.08455   \n",
       "567        20.60         29.33          140.10     1265.0          0.11780   \n",
       "568         7.76         24.54           47.92      181.0          0.05263   \n",
       "\n",
       "     compactness_mean  concavity_mean  concave points_mean  symmetry_mean  \\\n",
       "0             0.27760         0.30010              0.14710         0.2419   \n",
       "1             0.07864         0.08690              0.07017         0.1812   \n",
       "2             0.15990         0.19740              0.12790         0.2069   \n",
       "3             0.28390         0.24140              0.10520         0.2597   \n",
       "4             0.13280         0.19800              0.10430         0.1809   \n",
       "..                ...             ...                  ...            ...   \n",
       "564           0.11590         0.24390              0.13890         0.1726   \n",
       "565           0.10340         0.14400              0.09791         0.1752   \n",
       "566           0.10230         0.09251              0.05302         0.1590   \n",
       "567           0.27700         0.35140              0.15200         0.2397   \n",
       "568           0.04362         0.00000              0.00000         0.1587   \n",
       "\n",
       "     fractal_dimension_mean  ...  texture_worst  perimeter_worst  area_worst  \\\n",
       "0                   0.07871  ...          17.33           184.60      2019.0   \n",
       "1                   0.05667  ...          23.41           158.80      1956.0   \n",
       "2                   0.05999  ...          25.53           152.50      1709.0   \n",
       "3                   0.09744  ...          26.50            98.87       567.7   \n",
       "4                   0.05883  ...          16.67           152.20      1575.0   \n",
       "..                      ...  ...            ...              ...         ...   \n",
       "564                 0.05623  ...          26.40           166.10      2027.0   \n",
       "565                 0.05533  ...          38.25           155.00      1731.0   \n",
       "566                 0.05648  ...          34.12           126.70      1124.0   \n",
       "567                 0.07016  ...          39.42           184.60      1821.0   \n",
       "568                 0.05884  ...          30.37            59.16       268.6   \n",
       "\n",
       "     smoothness_worst  compactness_worst  concavity_worst  \\\n",
       "0             0.16220            0.66560           0.7119   \n",
       "1             0.12380            0.18660           0.2416   \n",
       "2             0.14440            0.42450           0.4504   \n",
       "3             0.20980            0.86630           0.6869   \n",
       "4             0.13740            0.20500           0.4000   \n",
       "..                ...                ...              ...   \n",
       "564           0.14100            0.21130           0.4107   \n",
       "565           0.11660            0.19220           0.3215   \n",
       "566           0.11390            0.30940           0.3403   \n",
       "567           0.16500            0.86810           0.9387   \n",
       "568           0.08996            0.06444           0.0000   \n",
       "\n",
       "     concave points_worst  symmetry_worst  fractal_dimension_worst  diagnosis  \n",
       "0                  0.2654          0.4601                  0.11890          M  \n",
       "1                  0.1860          0.2750                  0.08902          M  \n",
       "2                  0.2430          0.3613                  0.08758          M  \n",
       "3                  0.2575          0.6638                  0.17300          M  \n",
       "4                  0.1625          0.2364                  0.07678          M  \n",
       "..                    ...             ...                      ...        ...  \n",
       "564                0.2216          0.2060                  0.07115          M  \n",
       "565                0.1628          0.2572                  0.06637          M  \n",
       "566                0.1418          0.2218                  0.07820          M  \n",
       "567                0.2650          0.4087                  0.12400          M  \n",
       "568                0.0000          0.2871                  0.07039          B  \n",
       "\n",
       "[569 rows x 31 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "382ecabd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Malignanat 212\n",
      "Benign 357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/uca/.local/lib/python3.8/site-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import seaborn as sb\n",
    "sb.countplot(dataset['diagnosis'], label= 'Count')\n",
    "\n",
    "B,M = dataset['diagnosis'].value_counts()\n",
    "print('Malignanat', M)\n",
    "print('Benign', B)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "952820f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets now classify data into dependent and independent variables\n",
    "x = dataset.iloc[:, : -1]\n",
    "y = dataset.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "66f6a9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now lets convert the dependent variables into numerical variable\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d362b244",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data into training and testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a317b382",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.22609091,  0.14299357, -0.16219992, ...,  1.33438591,\n",
       "         1.22101459,  1.32977555],\n",
       "       [-0.28072076,  1.13113906, -0.34954245, ..., -0.81952682,\n",
       "        -0.77541863, -0.94570364],\n",
       "       [-0.04782508, -0.87231025, -0.12299829, ..., -0.49120548,\n",
       "        -1.31433312, -0.98696059],\n",
       "       ...,\n",
       "       [ 1.7233322 , -0.06173848,  1.70132185, ...,  1.51554921,\n",
       "         0.25341812, -0.26496405],\n",
       "       [ 1.18565945,  0.15552818,  1.16487847, ...,  0.53103066,\n",
       "         0.32690646, -0.37709831],\n",
       "       [ 0.24545096, -0.64668718,  0.25416267, ..., -0.19956228,\n",
       "        -1.2425945 , -0.01424877]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now lets scale our data using standard scaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "ss = StandardScaler()\n",
    "ss.fit_transform(x_train)\n",
    "ss.fit_transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "cce58aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets import our classifier and train our data and import dense\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "classifier = Sequential()\n",
    "\n",
    "#now we are making our first hidden layer \n",
    "\n",
    "#classifier.add(Dense(input_dim = 30, output_dim = 20, init='uniform',  activation='sigmoid'))\n",
    "classifier.add(Dense(20, input_dim=30, kernel_initializer='glorot_uniform', activation='sigmoid'))\n",
    "\n",
    "#lets add another hidden layer\n",
    "classifier.add(Dense(20, kernel_initializer='uniform', activation='sigmoid'))\n",
    "\n",
    "#now add third hidden layer\n",
    "classifier.add(Dense(20, kernel_initializer='uniform', activation='relu'))\n",
    "\n",
    "classifier.add(Dense(20, kernel_initializer='uniform', activation='tanh'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ae324b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 1s 6ms/step - loss: 0.6928 - accuracy: 0.5714\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.6915 - accuracy: 0.6374\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.6900 - accuracy: 0.6374\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.6881 - accuracy: 0.6374\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.6860 - accuracy: 0.6374\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.6831 - accuracy: 0.6374\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.6797 - accuracy: 0.6374\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.6755 - accuracy: 0.6374\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.6711 - accuracy: 0.6374\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.6654 - accuracy: 0.6374\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.6604 - accuracy: 0.6374\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.6550 - accuracy: 0.6374\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.6512 - accuracy: 0.6374\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.6497 - accuracy: 0.6374\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.6472 - accuracy: 0.6374\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.6458 - accuracy: 0.6374\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.6446 - accuracy: 0.6374\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.6433 - accuracy: 0.6374\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.6420 - accuracy: 0.6374\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.6405 - accuracy: 0.6374\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.6392 - accuracy: 0.6374\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.6376 - accuracy: 0.6374\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.6356 - accuracy: 0.6374\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.6337 - accuracy: 0.6374\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.6316 - accuracy: 0.6374\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.6293 - accuracy: 0.6374\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.6268 - accuracy: 0.6374\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.6239 - accuracy: 0.6374\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.6210 - accuracy: 0.6374\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.6180 - accuracy: 0.6374\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.6146 - accuracy: 0.6374\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.6104 - accuracy: 0.6374\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.6061 - accuracy: 0.6374\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.6018 - accuracy: 0.6374\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.5964 - accuracy: 0.6374\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.5911 - accuracy: 0.6374\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.5852 - accuracy: 0.6374\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.5791 - accuracy: 0.6374\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.5720 - accuracy: 0.6374\n",
      "Epoch 40/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.5642 - accuracy: 0.6374\n",
      "Epoch 41/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.5555 - accuracy: 0.6374\n",
      "Epoch 42/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.5474 - accuracy: 0.6374\n",
      "Epoch 43/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.5388 - accuracy: 0.6374\n",
      "Epoch 44/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.5273 - accuracy: 0.6374\n",
      "Epoch 45/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.5163 - accuracy: 0.6374\n",
      "Epoch 46/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.5025 - accuracy: 0.6374\n",
      "Epoch 47/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.4891 - accuracy: 0.6374\n",
      "Epoch 48/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.4813 - accuracy: 0.6374\n",
      "Epoch 49/100\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.4619 - accuracy: 0.6374\n",
      "Epoch 50/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.4499 - accuracy: 0.6374\n",
      "Epoch 51/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.4326 - accuracy: 0.8374\n",
      "Epoch 52/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.4148 - accuracy: 0.9055\n",
      "Epoch 53/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.4023 - accuracy: 0.8945\n",
      "Epoch 54/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3840 - accuracy: 0.8989\n",
      "Epoch 55/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3711 - accuracy: 0.9033\n",
      "Epoch 56/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3520 - accuracy: 0.9165\n",
      "Epoch 57/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3379 - accuracy: 0.9187\n",
      "Epoch 58/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3283 - accuracy: 0.9143\n",
      "Epoch 59/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3146 - accuracy: 0.9165\n",
      "Epoch 60/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3042 - accuracy: 0.9143\n",
      "Epoch 61/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.2951 - accuracy: 0.9187\n",
      "Epoch 62/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.2832 - accuracy: 0.9099\n",
      "Epoch 63/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.2788 - accuracy: 0.9187\n",
      "Epoch 64/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.2776 - accuracy: 0.9187\n",
      "Epoch 65/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.2678 - accuracy: 0.9077\n",
      "Epoch 66/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.2626 - accuracy: 0.9143\n",
      "Epoch 67/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.2541 - accuracy: 0.9165\n",
      "Epoch 68/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.2562 - accuracy: 0.9165\n",
      "Epoch 69/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.2428 - accuracy: 0.9231\n",
      "Epoch 70/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.2510 - accuracy: 0.9165\n",
      "Epoch 71/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.2363 - accuracy: 0.9231\n",
      "Epoch 72/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.2401 - accuracy: 0.9187\n",
      "Epoch 73/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.2416 - accuracy: 0.9231\n",
      "Epoch 74/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.2297 - accuracy: 0.9187\n",
      "Epoch 75/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.2288 - accuracy: 0.9143\n",
      "Epoch 76/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.2337 - accuracy: 0.9143\n",
      "Epoch 77/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.2260 - accuracy: 0.9231\n",
      "Epoch 78/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.2256 - accuracy: 0.9187\n",
      "Epoch 79/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.2290 - accuracy: 0.9187\n",
      "Epoch 80/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.2210 - accuracy: 0.9165\n",
      "Epoch 81/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.2172 - accuracy: 0.9231\n",
      "Epoch 82/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.2251 - accuracy: 0.9209\n",
      "Epoch 83/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.2119 - accuracy: 0.9231\n",
      "Epoch 84/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.2173 - accuracy: 0.9231\n",
      "Epoch 85/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.2192 - accuracy: 0.9253\n",
      "Epoch 86/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.2129 - accuracy: 0.9231\n",
      "Epoch 87/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.2104 - accuracy: 0.9231\n",
      "Epoch 88/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.2086 - accuracy: 0.9253\n",
      "Epoch 89/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.2070 - accuracy: 0.9209\n",
      "Epoch 90/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.2058 - accuracy: 0.9209\n",
      "Epoch 91/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.2073 - accuracy: 0.9187\n",
      "Epoch 92/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.2056 - accuracy: 0.9253\n",
      "Epoch 93/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.2103 - accuracy: 0.9143\n",
      "Epoch 94/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.2033 - accuracy: 0.9209\n",
      "Epoch 95/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.2039 - accuracy: 0.9209\n",
      "Epoch 96/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.2045 - accuracy: 0.9231\n",
      "Epoch 97/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.2040 - accuracy: 0.9231\n",
      "Epoch 98/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.2053 - accuracy: 0.9253\n",
      "Epoch 99/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.2011 - accuracy: 0.9275\n",
      "Epoch 100/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.2001 - accuracy: 0.9231\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5d96b12dc0>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lets add the output layer\n",
    "classifier.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n",
    "\n",
    "#compiling, adding loss function and also adding accuracy metrics\n",
    "classifier.compile(optimizer=\"Adam\", loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#fitting data while speciying batch size and epochs\n",
    "classifier.fit(x_train, y_train, batch_size=100, epochs=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "97de1dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the Test set results\n",
    "y_predict = classifier.predict(x_test)\n",
    "y_predict = (y_predict > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e807ef45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[62  5]\n",
      " [ 7 40]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plt.figure(2)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_predict)\n",
    "sb.heatmap(cm,annot=True)\n",
    "plt.savefig('h.png')\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c53eef83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.93      0.91        67\n",
      "           1       0.89      0.85      0.87        47\n",
      "\n",
      "    accuracy                           0.89       114\n",
      "   macro avg       0.89      0.89      0.89       114\n",
      "weighted avg       0.89      0.89      0.89       114\n",
      "\n",
      "[[62  5]\n",
      " [ 7 40]]\n"
     ]
    }
   ],
   "source": [
    "#now find report using confusion metrics\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "print(classification_report(y_test,y_predict))\n",
    "print(confusion_matrix(y_test,y_predict))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
